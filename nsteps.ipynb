{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-step Q-learning\n",
    "\n",
    "In this part, we implement the N-step variant of the Q-learning algorithm, using a replay buffer as we did for studying the off-policy property of Q-learning.\n",
    "\n",
    "Using the N-Steps return accelerates the learning process by updating the Q values of $N-1$ states at a time, by recording together succesive states visited by the agent.\n",
    "\n",
    "In this lab, we focus on updating the states visited by the agent when following its policy until he reached its current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class NStepSample\n",
    "\n",
    "An N-step sample contains N samples, corresponding to N consecutive steps of an agent.\n",
    "\n",
    "In the cell below, import the Sample class from the [on_off_policy.ipynb](on_off_policy.ipynb) notebook and create a class to store an N-step sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"define NSTepSample class here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is necessary, adapt your class ReplayBuffer so that it can contain N-step samples instead of simple samples. Maybe no adaptation is necessary if you make so that the class NStepSample inherits from Sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"put your modified ReplayBuffer class here, or import the one from on_off_policy notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the necessary code so that an agent running a specific policy can fill a replay buffer with the steps it is taking in the environment. After its N-1 first steps, each time the agent takes a step, it adds an N-step sample to the replay buffer. Thus the replay buffer contains samples about the same steps several times, but shifted.\n",
    "\n",
    "![N-step-return|50x50](N-step-return.png)\n",
    "\n",
    "Create a function which, given a policy, a replay buffer and a number of steps N (take for instance N = 5) runs a simulation of the agent in its environment.\n",
    "    \n",
    "After the agent's N-1 first steps, start saving, at each step, the N data (state + action + reward) gathered into the replay buffer. In order to do that, use the NStepSample objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"put your code to fill the replay buffer here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy-paste your modified Q-learning code from the [on_off_policy.ipynb](on_off_policy.ipynb) notebook.\n",
    "Modify it again so that it takes N-step samples as input rather than single samples.\n",
    "\n",
    "Hint : You can generate two Q-tables in your Q-Learning algorithm, one that performs the nsteps update and one that doesn't, this will help you analyze your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"put your modified Q-learning code here and adapt it to the N-step samples case\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write a code that:\n",
    "\n",
    "* creates a replay buffer with a size of 10.000 samples, fills it with N-step samples coming from an agent running one or several different policies,\n",
    "* makes Q-learning learn from samples drawn uniformly from this replay buffer, until a fixed number of episodes ,\n",
    "* visualizes the results.\n",
    "\n",
    " \n",
    "Hint : you can play with your algorithm, using different policies when generating your samples, updating your ReplayBuffer in the Q-Learning algorithm above with newly generated samples from your policy and increasing either the size of the buffer or the number of episodes. You can then compare the effectiveness of the different variations of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"put your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c31ff3549c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# visualize your results\n",
    "%matplotlib inline\n",
    "\n",
    "m.new_render()\n",
    "m.render(Q_list[-1], pol_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
